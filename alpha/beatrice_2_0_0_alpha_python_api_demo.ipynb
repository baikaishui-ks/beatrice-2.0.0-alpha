{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO_jw6vTXagm"
   },
   "source": [
    "# Beatrice 2.0.0-alpha (Python API)\n",
    "\n",
    "## Beatrice 2.0 について\n",
    "\n",
    "Beatrice は、**超低遅延・低負荷・低容量**を特徴とする**完全無料**の AI ボイスチェンジャー VST であり、最初のメジャーリリースであるバージョン 1.0.0 は、低遅延・低負荷な AI ボイスチェンジャーの先駆けとして 2023 年 9 月 10 日にリリースされました。\n",
    "\n",
    "Beatrice 2 では、これまでの Beatrice の構成を全て見直し、**大幅な性能強化**と**利便性の向上**を目指しています。\n",
    "\n",
    "### Beatrice 2 のゴール\n",
    "\n",
    "* **自分の変換された声を聴きながら、歌を快適に歌えるようにする**\n",
    "* **入力された声の抑揚を変換音声に正確に反映し、より繊細な表現を可能にする**\n",
    "* **変換音声のより高い自然性と明瞭さ**\n",
    "* **より多様な変換先話者**\n",
    "* 50ms 程度の遅延\n",
    "  * これまでの Beatrice と同程度\n",
    "  * 外部の録音機器で実測した場合の値。デバイスによる遅延などを含めない場合は、計算方法により異なるが、35ms 程度が目安。\n",
    "* 開発者のノート PC (Intel Core i7-1165G7) でシングルスレッドで動作させ、RTF < 0.25 となる程度の負荷\n",
    "  * これまでの Beatrice と同程度か、より低負荷\n",
    "* 30MB 以下の容量\n",
    "  * これまでの Beatrice と同程度か、より低容量\n",
    "* (快適な開発)\n",
    "  * これまでより少ない依存データ、単純なソースコード、再現性のある実験など内部的な話\n",
    "* その他 (内緒)\n",
    "\n",
    "## このデモについて\n",
    "\n",
    "リアルタイムで変換を行うための API の実装ができましたが、クライアントの作成にまだ時間がかかるため、API とその使用例を公開して進捗を示すものです。\n",
    "\n",
    "モデルは開発途上であり、品質やピッチへの追従性、ノイズ耐性などはまだまだ向上する見込みがあります。\n",
    "<small>改善案に計算リソースが追い付いていません</small>\n",
    "\n",
    "注意点として、バグの除去は十分ではありません。\n",
    "例えば、不適切なパラメータファイルを読み込むと例外は出ずに Python ごと落ちます。\n",
    "\n",
    "また、このデモを使用したことによる使用者の不利益に対して、Project Beatrice およびその関係者は一切の責任を負いません。\n",
    "\n",
    "## ライセンス\n",
    "\n",
    "### 重み・出力音声\n",
    "\n",
    "重み (.bin) と出力された変換音声は、JVS corpus と JVS-MuSiC のライセンスに従って使ってください。\n",
    "Beatrice 2.0.0-alpha を使用していることを記載する必要はありません。\n",
    "\n",
    "### ライブラリ・この notebook\n",
    "\n",
    "alpha 版のライブラリ (.so ファイル) やこの使用例の Python コードもご自由に再利用いただけますが、Beatrice 2.0.0-alpha (Python API) を利用していることを記載してください。\n",
    "<small>(もっとも、バグがどこにあるかわからないのでまともに使うのは難しいと思います。)</small>\n",
    "後述の使用ライブラリのライセンスもご確認ください。\n",
    "\n",
    "また、この Python API は C 言語の API (Windows/Linux) のラッパーとして作られており、遅延を切り詰めたクライアントを作る場合にはガベージコレクタが無い C/C++ などでの実装が有利と考えられます。\n",
    "使いたい場合は Project Beatrice にご連絡ください。\n",
    "<small>というか C/C++ 書けるなら公式のクライアントを作るのを手伝ってほしいです。</small>\n",
    "\n",
    "## リソース\n",
    "\n",
    "* JVS corpus: https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_corpus\n",
    "* JVS-MuSiC: https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_music\n",
    "* PocketFFT: https://gitlab.mpcdf.mpg.de/mtr/pocketfft/-/tree/cpp\n",
    "  * Copyright 2010-2018 Max-Planck-Society\n",
    "  * BSD-3-Clause license\n",
    "  * https://opensource.org/license/bsd-3-clause/\n",
    "* fmath: https://github.com/herumi/fmath\n",
    "  * Copyright 2009 MITSUNARI Shigeo\n",
    "  * BSD-3-Clause license\n",
    "  * https://opensource.org/license/bsd-3-clause/\n",
    "* NumPy: https://github.com/numpy/numpy\n",
    "  * Copyright 2005-2023 NumPy Developers\n",
    "  * BSD-3-Clause license\n",
    "  * https://opensource.org/license/bsd-3-clause/\n",
    "* Python: https://github.com/python/cpython\n",
    "  * Copyright 2001-2023 Python Software Foundation\n",
    "  * Copyright 2000 BeOpen.com\n",
    "  * Copyright 1995-2000 Corporation for National Research Initiatives\n",
    "  * Copyright 1991-1995 Stichting Mathematisch Centrum\n",
    "  * PSF License\n",
    "  * https://docs.python.org/3.10/license.html#psf-license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 下準備\n",
    "\n",
    "# @markdown セルを上から順に実行すると、最後のセルで変換が行われます。\n",
    "# @markdown パラメータを変更した場合は、変更したセル以降を順に実行してください。\n",
    "\n",
    "!git clone -b alpha https://huggingface.co/fierce-cats/beatrice-2.0.0-alpha\n",
    "!cp beatrice-2.0.0-alpha/alpha/* .\n",
    "!pip install pyworld\n",
    "\n",
    "import math\n",
    "import random\n",
    "from base64 import b64decode\n",
    "from time import perf_counter\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyworld\n",
    "from google.colab import output\n",
    "from IPython.display import Audio, Javascript\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from beatrice import (\n",
    "    IN_HOP_LENGTH,\n",
    "    OUT_HOP_LENGTH,\n",
    "    IN_SAMPLE_RATE,\n",
    "    OUT_SAMPLE_RATE,\n",
    "    PITCH_BINS,\n",
    "    PITCH_BINS_PER_OCTAVE,\n",
    "    PhoneExtractor,\n",
    "    PitchEstimator,\n",
    "    WaveformGenerator,\n",
    "    read_speaker_embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{IN_SAMPLE_RATE=}\")\n",
    "print(f\"{OUT_SAMPLE_RATE=}\")\n",
    "\n",
    "RECORD = \"\"\"\n",
    "const sleep  = time => new Promise(resolve => setTimeout(resolve, time));\n",
    "const b2text = blob => new Promise(resolve => {\n",
    "  const reader = new FileReader();\n",
    "  reader.onloadend = e => resolve(e.srcElement.result);\n",
    "  reader.readAsDataURL(blob);\n",
    "});\n",
    "\n",
    "var record = time => new Promise(async resolve => {\n",
    "  const div = document.createElement('div');\n",
    "  const button = document.createElement('button');\n",
    "\n",
    "  button.textContent = \"Start Recording\";\n",
    "  button.onclick = async function(){\n",
    "    button.disabled = true;\n",
    "    button.textContent = \"Recording...\";\n",
    "    stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "    recorder = new MediaRecorder(stream);\n",
    "    chunks = [];\n",
    "    recorder.ondataavailable = e => chunks.push(e.data);\n",
    "    recorder.start();\n",
    "    await sleep(time);\n",
    "    recorder.onstop = async ()=>{\n",
    "      blob = new Blob(chunks);\n",
    "      text = await b2text(blob);\n",
    "      resolve(text);\n",
    "      button.textContent = \"Recording stopped.\";\n",
    "    }\n",
    "    recorder.stop();\n",
    "  }\n",
    "  div.appendChild(button);\n",
    "\n",
    "  document.querySelector(\"#output-area\").appendChild(div);\n",
    "});\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def record(sec=3):\n",
    "    # adapted from https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be\n",
    "    display(Javascript(RECORD))\n",
    "    s = output.eval_js(f\"record({sec * 1000})\")\n",
    "    b = b64decode(s.split(\",\")[1])\n",
    "    with open(\"recording.wav\",'wb') as f:\n",
    "        f.write(b)\n",
    "\n",
    "\n",
    "def slerp(t, v0, v1):\n",
    "    # adapted from https://gist.github.com/dvschultz/3af50c40df002da3b751efab1daddf2c\n",
    "    v0_copy = v0\n",
    "    v1_copy = v1\n",
    "    v0 = v0 / np.linalg.norm(v0)\n",
    "    v1 = v1 / np.linalg.norm(v1)\n",
    "    dot = np.sum(v0 * v1)\n",
    "    if np.abs(dot) > 0.9995:\n",
    "        return v0_copy * (1.0 - t) + v1_copy * t\n",
    "    theta_0 = np.arccos(dot)\n",
    "    sin_theta_0 = np.sin(theta_0)\n",
    "    theta_t = theta_0 * t\n",
    "    sin_theta_t = np.sin(theta_t)\n",
    "    s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
    "    s1 = sin_theta_t / sin_theta_0\n",
    "    v2 = s0 * v0_copy + s1 * v1_copy\n",
    "    return v2\n",
    "\n",
    "\n",
    "target_speaker_names = [f\"jvs{i:03d}\" for i in range(1, 101)]\n",
    "target_speaker_mean_pitches = [146, 216, 154, 248, 140, 100, 208, 228, 122, 278, 156, 119, 148, 274, 256, 199, 213, 217, 233, 137, 109, 120, 125, 248, 224, 257, 238, 117, 220, 250, 123, 171, 144, 119, 212, 250, 109, 227, 244, 230, 144, 131, 212, 131, 137, 136, 138, 110, 133, 132, 241, 151, 227, 138, 230, 217, 212, 220, 226, 229, 247, 215, 232, 209, 243, 247, 246, 126, 225, 133, 112, 231, 142, 117, 151, 144, 154, 110, 134, 154, 132, 217, 246, 218, 239, 132, 151, 130, 122, 264, 215, 203, 267, 218, 225, 242, 163, 178, 158, 125]\n",
    "\n",
    "nearest_c_major_scale = np.zeros(PITCH_BINS, dtype=np.int32)\n",
    "i = 0\n",
    "while i < PITCH_BINS:\n",
    "    for s in [2, 1, 2, 2, 1, 2, 2]:\n",
    "        i += s * 8\n",
    "        if i >= PITCH_BINS:\n",
    "            break\n",
    "        nearest_c_major_scale[i] = i\n",
    "for _ in range(20):\n",
    "    for i in range(1, PITCH_BINS - 1):\n",
    "        nearest_c_major_scale[i] = nearest_c_major_scale[i] or nearest_c_major_scale[i - 1] or nearest_c_major_scale[i + 1]\n",
    "nearest_c_major_scale[-1] = nearest_c_major_scale[-2]\n",
    "\n",
    "phone_extractor_parameter_file = \"phone_extractor_003b_checkpoint_03000000.bin\"\n",
    "pitch_estimator_parameter_file = \"pitch_estimator_008_1_checkpoint_00300000.bin\"\n",
    "speaker_embeddings_file = \"speaker_embeddings_017_checkpoint_01300000.bin\"\n",
    "waveform_generator_parameter_file = \"waveform_generator_017_checkpoint_01300000.bin\"\n",
    "\n",
    "phone_extractor = PhoneExtractor()\n",
    "phone_extractor.read_parameters(phone_extractor_parameter_file)\n",
    "pitch_estimator = PitchEstimator()\n",
    "pitch_estimator.read_parameters(pitch_estimator_parameter_file)\n",
    "speaker_embeddings = read_speaker_embeddings(speaker_embeddings_file)\n",
    "waveform_generator = WaveformGenerator()\n",
    "waveform_generator.read_parameters(waveform_generator_parameter_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 入力音声\n",
    "\n",
    "# @markdown `record_your_voice` を選択してセルを実行すると、録音ボタンが表示されます。\n",
    "\n",
    "in_filename = \"jvs003_16k.wav\"  # @param [\"jvs001_16k.wav\", \"jvs002_16k.wav\", \"jvs003_16k.wav\", \"jvs004_16k.wav\", \"record_your_voice\"] {allow-input: true}\n",
    "if in_filename == \"record_your_voice\":\n",
    "    record(6)\n",
    "    in_filename = \"recording.wav\"\n",
    "\n",
    "source_wav, _ = librosa.load(in_filename, sr=IN_SAMPLE_RATE)\n",
    "assert source_wav.ndim == 1\n",
    "source_wav = source_wav.astype(np.float32)\n",
    "display(Audio(source_wav, rate=IN_SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 変換の設定\n",
    "\n",
    "target_speaker = \"random\"  # @param [\"random\", \"jvs001\", \"jvs002\", \"jvs003\", \"jvs004\", \"jvs005\", \"jvs006\", \"jvs007\", \"jvs008\", \"jvs009\", \"jvs010\", \"jvs011\", \"jvs012\", \"jvs013\", \"jvs014\", \"jvs015\", \"jvs016\", \"jvs017\", \"jvs018\", \"jvs019\", \"jvs020\", \"jvs021\", \"jvs022\", \"jvs023\", \"jvs024\", \"jvs025\", \"jvs026\", \"jvs027\", \"jvs028\", \"jvs029\", \"jvs030\", \"jvs031\", \"jvs032\", \"jvs033\", \"jvs034\", \"jvs035\", \"jvs036\", \"jvs037\", \"jvs038\", \"jvs039\", \"jvs040\", \"jvs041\", \"jvs042\", \"jvs043\", \"jvs044\", \"jvs045\", \"jvs046\", \"jvs047\", \"jvs048\", \"jvs049\", \"jvs050\", \"jvs051\", \"jvs052\", \"jvs053\", \"jvs054\", \"jvs055\", \"jvs056\", \"jvs057\", \"jvs058\", \"jvs059\", \"jvs060\", \"jvs061\", \"jvs062\", \"jvs063\", \"jvs064\", \"jvs065\", \"jvs066\", \"jvs067\", \"jvs068\", \"jvs069\", \"jvs070\", \"jvs071\", \"jvs072\", \"jvs073\", \"jvs074\", \"jvs075\", \"jvs076\", \"jvs077\", \"jvs078\", \"jvs079\", \"jvs080\", \"jvs081\", \"jvs082\", \"jvs083\", \"jvs084\", \"jvs085\", \"jvs086\", \"jvs087\", \"jvs088\", \"jvs089\", \"jvs090\", \"jvs091\", \"jvs092\", \"jvs093\", \"jvs094\", \"jvs095\", \"jvs096\", \"jvs097\", \"jvs098\", \"jvs099\", \"jvs100\"]\n",
    "pitch_shift_semitones_ = 'auto'  # @param [\"'auto'\", \"-12\", \"-10\", \"-8\", \"-6\", \"-4\", \"-2\", \"0\", \"2\", \"4\", \"6\", \"8\", \"10\", \"12\"] {type:\"raw\", allow-input: true}\n",
    "second_target_speaker = \"none\"  # @param [\"none\", \"random\", \"jvs001\", \"jvs002\", \"jvs003\", \"jvs004\", \"jvs005\", \"jvs006\", \"jvs007\", \"jvs008\", \"jvs009\", \"jvs010\", \"jvs011\", \"jvs012\", \"jvs013\", \"jvs014\", \"jvs015\", \"jvs016\", \"jvs017\", \"jvs018\", \"jvs019\", \"jvs020\", \"jvs021\", \"jvs022\", \"jvs023\", \"jvs024\", \"jvs025\", \"jvs026\", \"jvs027\", \"jvs028\", \"jvs029\", \"jvs030\", \"jvs031\", \"jvs032\", \"jvs033\", \"jvs034\", \"jvs035\", \"jvs036\", \"jvs037\", \"jvs038\", \"jvs039\", \"jvs040\", \"jvs041\", \"jvs042\", \"jvs043\", \"jvs044\", \"jvs045\", \"jvs046\", \"jvs047\", \"jvs048\", \"jvs049\", \"jvs050\", \"jvs051\", \"jvs052\", \"jvs053\", \"jvs054\", \"jvs055\", \"jvs056\", \"jvs057\", \"jvs058\", \"jvs059\", \"jvs060\", \"jvs061\", \"jvs062\", \"jvs063\", \"jvs064\", \"jvs065\", \"jvs066\", \"jvs067\", \"jvs068\", \"jvs069\", \"jvs070\", \"jvs071\", \"jvs072\", \"jvs073\", \"jvs074\", \"jvs075\", \"jvs076\", \"jvs077\", \"jvs078\", \"jvs079\", \"jvs080\", \"jvs081\", \"jvs082\", \"jvs083\", \"jvs084\", \"jvs085\", \"jvs086\", \"jvs087\", \"jvs088\", \"jvs089\", \"jvs090\", \"jvs091\", \"jvs092\", \"jvs093\", \"jvs094\", \"jvs095\", \"jvs096\", \"jvs097\", \"jvs098\", \"jvs099\", \"jvs100\"]\n",
    "second_target_mode = \"morph\"  # @param [\"merge\", \"morph\"]\n",
    "bonus = \"none\"  # @param [\"none\", \"vibrato\", \"autotune\", \"monotone\"]\n",
    "block_size = 1  # 現在は最も遅延が短くなる 1 (10ms) のみ対応\n",
    "\n",
    "# 以下は設定値を API に渡すための前処理\n",
    "\n",
    "if target_speaker == \"random\":\n",
    "    target_speaker_id = random.randint(0, 99)\n",
    "else:\n",
    "    target_speaker_id = target_speaker_names.index(target_speaker)\n",
    "print(f\"{target_speaker_id=}\")\n",
    "print(f\"target speaker name: {target_speaker_names[target_speaker_id]}\")\n",
    "if second_target_speaker == \"none\":\n",
    "    second_target_speaker_id = target_speaker_id\n",
    "elif second_target_speaker == \"random\":\n",
    "    second_target_speaker_id = random.randint(0, 99)\n",
    "else:\n",
    "    second_target_speaker_id = target_speaker_names.index(second_target_speaker)\n",
    "print(f\"{second_target_speaker_id=}\")\n",
    "print(f\"second target speaker name: {target_speaker_names[second_target_speaker_id]}\")\n",
    "\n",
    "if pitch_shift_semitones_ == \"auto\":\n",
    "    target_speaker_mean_pitch = target_speaker_mean_pitches[target_speaker_id]\n",
    "    second_target_speaker_mean_pitch = target_speaker_mean_pitches[second_target_speaker_id]\n",
    "    tmp_source_pitch, _ = pyworld.harvest(source_wav.astype(np.float64), IN_SAMPLE_RATE)\n",
    "    tmp_source_pitch = tmp_source_pitch[tmp_source_pitch > 0]\n",
    "    if len(tmp_source_pitch) == 0:\n",
    "        pitch_shift_semitones = 0.0\n",
    "        second_pitch_shift_semitones = 0.0\n",
    "    else:\n",
    "        source_speaker_mean_pitch = math.exp(float(np.log(tmp_source_pitch).mean()))\n",
    "        pitch_shift_semitones = math.log2(target_speaker_mean_pitch / source_speaker_mean_pitch) * 12.0\n",
    "        second_pitch_shift_semitones = math.log2(second_target_speaker_mean_pitch / source_speaker_mean_pitch) * 12.0\n",
    "else:\n",
    "    pitch_shift_semitones = float(pitch_shift_semitones_)\n",
    "    second_pitch_shift_semitones = pitch_shift_semitones\n",
    "pitch_shift_semitones = round(pitch_shift_semitones, 1)\n",
    "second_pitch_shift_semitones = round(second_pitch_shift_semitones, 1)\n",
    "print(f\"{pitch_shift_semitones=}\")\n",
    "print(f\"{second_pitch_shift_semitones=}\")\n",
    "\n",
    "if second_target_mode == \"merge\":\n",
    "    target_speaker_embeddings = slerp(\n",
    "        speaker_embeddings[target_speaker_id],\n",
    "        speaker_embeddings[second_target_speaker_id],\n",
    "        0.5,\n",
    "    )\n",
    "    target_speaker_embeddings = np.stack([target_speaker_embeddings] * 400)\n",
    "    quantized_pitch_shift = np.array([int(round(\n",
    "        (pitch_shift_semitones + second_pitch_shift_semitones) * 0.5 * PITCH_BINS_PER_OCTAVE / 12.0\n",
    "    ))] * 400, dtype=np.int32)\n",
    "else:\n",
    "    target_speaker_embeddings = []\n",
    "    quantized_pitch_shift = []\n",
    "    target_speaker_embeddings += [speaker_embeddings[target_speaker_id]] * 100\n",
    "    quantized_pitch_shift += [int(round(pitch_shift_semitones * PITCH_BINS_PER_OCTAVE / 12.0))] * 100\n",
    "    for i in range(100):\n",
    "        target_speaker_embeddings.append(slerp(\n",
    "            i / 100.0,\n",
    "            speaker_embeddings[target_speaker_id],\n",
    "            speaker_embeddings[second_target_speaker_id],\n",
    "        ))\n",
    "        quantized_pitch_shift.append(int(round(\n",
    "            (pitch_shift_semitones * (100 - i) + second_pitch_shift_semitones * i) / 100.0\n",
    "            * PITCH_BINS_PER_OCTAVE / 12.0\n",
    "        )))\n",
    "    target_speaker_embeddings += [speaker_embeddings[second_target_speaker_id]] * 100\n",
    "    quantized_pitch_shift += [int(round(second_pitch_shift_semitones * PITCH_BINS_PER_OCTAVE / 12.0))] * 100\n",
    "    for i in range(100):\n",
    "        target_speaker_embeddings.append(slerp(\n",
    "            i / 100.0,\n",
    "            speaker_embeddings[second_target_speaker_id],\n",
    "            speaker_embeddings[target_speaker_id],\n",
    "        ))\n",
    "        quantized_pitch_shift.append(int(round(\n",
    "            (second_pitch_shift_semitones * (100 - i) + pitch_shift_semitones * i) / 100.0\n",
    "            * PITCH_BINS_PER_OCTAVE / 12.0\n",
    "        )))\n",
    "    target_speaker_embeddings = np.stack(target_speaker_embeddings)\n",
    "    quantized_pitch_shift = np.array(quantized_pitch_shift, dtype=np.int32)\n",
    "print(f\"{target_speaker_embeddings.shape=}\")\n",
    "print(f\"{quantized_pitch_shift=}\")\n",
    "\n",
    "if bonus == \"vibrato\":\n",
    "    vibrato_rate_hz = 4.0\n",
    "    vibrato_depth_semitones = 3.0\n",
    "    vibrato_wave = np.sin(np.linspace(\n",
    "        0, 2.0 * math.pi * vibrato_rate_hz * 400 / 100, 400, endpoint=False\n",
    "    )) * (vibrato_depth_semitones * PITCH_BINS_PER_OCTAVE / 12.0)\n",
    "    quantized_pitch_shift += vibrato_wave.round().astype(np.int32)\n",
    "    print(f\"{quantized_pitch_shift=}\")\n",
    "elif bonus == \"autotune\":\n",
    "    print(f\"{nearest_c_major_scale=}\")\n",
    "elif bonus == \"monotone\":\n",
    "    quantized_target_pitch = int(round(\n",
    "        math.log2(math.sqrt(target_speaker_mean_pitch * second_target_speaker_mean_pitch) / 55.0) * PITCH_BINS_PER_OCTAVE\n",
    "    ))\n",
    "    print(f\"{quantized_target_pitch=}\")\n",
    "elif bonus == \"whisper\":\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 変換\n",
    "\n",
    "assert block_size == 1\n",
    "phone_ctx = phone_extractor.new_context(block_size)\n",
    "pitch_ctx = pitch_estimator.new_context(block_size)\n",
    "waveform_ctx = waveform_generator.new_context(block_size)\n",
    "\n",
    "block_size_in_input_sample_rate = block_size * IN_HOP_LENGTH\n",
    "\n",
    "converted_wav_segments = []\n",
    "t0 = perf_counter()\n",
    "\n",
    "# block_size * 10ms ずつ変換する\n",
    "for left in tqdm(range(0, len(source_wav), block_size_in_input_sample_rate)):\n",
    "    source_wav_segment = source_wav[left : left + block_size_in_input_sample_rate]\n",
    "    if len(source_wav_segment) < block_size_in_input_sample_rate:\n",
    "        source_wav_segment = np.pad(\n",
    "            source_wav_segment,\n",
    "            (0, block_size_in_input_sample_rate - len(source_wav_segment)),\n",
    "        )\n",
    "    phone = phone_extractor(source_wav_segment, phone_ctx)\n",
    "    quantized_pitch, pitch_features = pitch_estimator(source_wav_segment, pitch_ctx)\n",
    "    l = left // block_size_in_input_sample_rate % 400\n",
    "    if bonus in {\"none\", \"vibrato\", \"autotune\"}:\n",
    "        quantized_pitch = (\n",
    "            quantized_pitch\n",
    "            + quantized_pitch_shift[l : l + block_size]\n",
    "        ).clip(1, PITCH_BINS - 1)\n",
    "        if bonus == \"autotune\":\n",
    "            quantized_pitch = nearest_c_major_scale[quantized_pitch]\n",
    "    elif bonus == \"monotone\":\n",
    "        quantized_pitch[:] = quantized_target_pitch\n",
    "    else:\n",
    "        assert False\n",
    "    speaker_embedding = target_speaker_embeddings[l : l + block_size]\n",
    "    converted_wav_segment = waveform_generator(\n",
    "        phone, quantized_pitch, pitch_features, speaker_embedding, waveform_ctx\n",
    "    )\n",
    "    converted_wav_segments.append(converted_wav_segment)\n",
    "elapsed_time = perf_counter() - t0\n",
    "rtf = elapsed_time / len(source_wav) * IN_SAMPLE_RATE\n",
    "print(f\"Elapsed time: {elapsed_time:.3f}s\")\n",
    "print(f\"RTF: {rtf:.3f}\")  # Xeon 遅すぎ！\n",
    "\n",
    "converted_wav = np.concatenate(converted_wav_segments)\n",
    "converted_wav[:2000] *= np.linspace(0.0, 1.0, 2000)\n",
    "\n",
    "plt.figure(figsize=(16, 1))\n",
    "plt.plot(np.arange(len(source_wav)) / IN_SAMPLE_RATE, source_wav + 0.1, label=\"source\")\n",
    "plt.plot(np.arange(len(converted_wav)) / OUT_SAMPLE_RATE, converted_wav - 0.1, label=\"converted\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "display(Audio(converted_wav, rate=OUT_SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
